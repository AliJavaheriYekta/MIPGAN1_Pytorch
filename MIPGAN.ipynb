{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MIPGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "405b5797cf9445d4accd2fe7db14a6c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_70b5193785d04ca2ac83a8f14148f2a1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_07a45dc26b1f422882c6bf95223a65f3",
              "IPY_MODEL_e4e3550840fd4cdbbb8fe3c2419c4e46"
            ]
          }
        },
        "70b5193785d04ca2ac83a8f14148f2a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "07a45dc26b1f422882c6bf95223a65f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_848f64ec2b074f05b39201bb483d0d24",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_28ac45ccb0834e569012c7c92444a9b1"
          }
        },
        "e4e3550840fd4cdbbb8fe3c2419c4e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1fa6b672a83d472aa60b720dabb7908d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 210MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_acfeb437210f412d94624bf49374ee95"
          }
        },
        "848f64ec2b074f05b39201bb483d0d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "28ac45ccb0834e569012c7c92444a9b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1fa6b672a83d472aa60b720dabb7908d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "acfeb437210f412d94624bf49374ee95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1603d316f44e4f4baf5ca94a5d2b3e68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dbb962606deb47aea5d94fa232ad7aab",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_91dd85398c5c446b8bc96f9d95ad1157",
              "IPY_MODEL_3d0a91529bb34065bed6361377850e54"
            ]
          }
        },
        "dbb962606deb47aea5d94fa232ad7aab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "91dd85398c5c446b8bc96f9d95ad1157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_23f7c750af9943a1888451967deb4524",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31f9824305e14f238694ee78b101df4a"
          }
        },
        "3d0a91529bb34065bed6361377850e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d038b4fa067a47a6b506f62683714e10",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:06&lt;00:00, 87.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9625cbf8384548e1a33744bb3db6c1e5"
          }
        },
        "23f7c750af9943a1888451967deb4524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31f9824305e14f238694ee78b101df4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d038b4fa067a47a6b506f62683714e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9625cbf8384548e1a33744bb3db6c1e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMH0_lGkpoqn",
        "outputId": "e8dcd6c9-ab6d-4c23-9918-a9d6bd9baefa"
      },
      "source": [
        "! git clone \"https://github.com/pacifinapacific/StyleGAN_LatentEditor\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'StyleGAN_LatentEditor'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 170 (delta 0), reused 0 (delta 0), pack-reused 167\u001b[K\n",
            "Receiving objects: 100% (170/170), 4.26 MiB | 13.33 MiB/s, done.\n",
            "Resolving deltas: 100% (70/70), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWL5fMmVFq88",
        "outputId": "67c74f20-2525-4f42-8f83-ea3c757b16f3"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "149.png  69660.png  69959.png  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgbf56YZWMo7",
        "outputId": "97fead87-5956-4ea9-e30c-cb925935e7c7"
      },
      "source": [
        "cd StyleGAN_LatentEditor\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/StyleGAN_LatentEditor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy-ncx32sB2x",
        "outputId": "3f21e701-f1f1-466c-c203-f2dd987fe461"
      },
      "source": [
        "%cd StyleGAN_LatentEditor/weight_files/pytorch\r\n",
        "! wget https://github.com/lernapparat/lernapparat/releases/download/v2019-02-01/karras2019stylegan-ffhq-1024x1024.for_g_all.pt\r\n",
        "%cd ..\r\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/StyleGAN_LatentEditor/weight_files/pytorch\n",
            "--2021-03-08 11:48:43--  https://github.com/lernapparat/lernapparat/releases/download/v2019-02-01/karras2019stylegan-ffhq-1024x1024.for_g_all.pt\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/172400888/2bf3e000-389b-11e9-9435-af76af69c167?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210308%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210308T114843Z&X-Amz-Expires=300&X-Amz-Signature=ec6dba4a4f3e758f356a50c055f05d225f4b1b6302d1ac2be51f6cbd6b9584c6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=172400888&response-content-disposition=attachment%3B%20filename%3Dkarras2019stylegan-ffhq-1024x1024.for_g_all.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-03-08 11:48:43--  https://github-releases.githubusercontent.com/172400888/2bf3e000-389b-11e9-9435-af76af69c167?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210308%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210308T114843Z&X-Amz-Expires=300&X-Amz-Signature=ec6dba4a4f3e758f356a50c055f05d225f4b1b6302d1ac2be51f6cbd6b9584c6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=172400888&response-content-disposition=attachment%3B%20filename%3Dkarras2019stylegan-ffhq-1024x1024.for_g_all.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 104878232 (100M) [application/octet-stream]\n",
            "Saving to: ‘karras2019stylegan-ffhq-1024x1024.for_g_all.pt’\n",
            "\n",
            "karras2019stylegan- 100%[===================>] 100.02M  62.1MB/s    in 1.6s    \n",
            "\n",
            "2021-03-08 11:48:45 (62.1 MB/s) - ‘karras2019stylegan-ffhq-1024x1024.for_g_all.pt’ saved [104878232/104878232]\n",
            "\n",
            "/content/StyleGAN_LatentEditor/weight_files\n",
            "/content/StyleGAN_LatentEditor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQnDDSq8pnQk"
      },
      "source": [
        "!mkdir save_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOpvMVcsqUTS"
      },
      "source": [
        "!mkdir save_image/morph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "405b5797cf9445d4accd2fe7db14a6c9",
            "70b5193785d04ca2ac83a8f14148f2a1",
            "07a45dc26b1f422882c6bf95223a65f3",
            "e4e3550840fd4cdbbb8fe3c2419c4e46",
            "848f64ec2b074f05b39201bb483d0d24",
            "28ac45ccb0834e569012c7c92444a9b1",
            "1fa6b672a83d472aa60b720dabb7908d",
            "acfeb437210f412d94624bf49374ee95",
            "1603d316f44e4f4baf5ca94a5d2b3e68",
            "dbb962606deb47aea5d94fa232ad7aab",
            "91dd85398c5c446b8bc96f9d95ad1157",
            "3d0a91529bb34065bed6361377850e54",
            "23f7c750af9943a1888451967deb4524",
            "31f9824305e14f238694ee78b101df4a",
            "d038b4fa067a47a6b506f62683714e10",
            "9625cbf8384548e1a33744bb3db6c1e5"
          ]
        },
        "id": "VD3_iZjtoKkp",
        "outputId": "382492ac-94ed-45cd-ca14-423a2d0f56f0"
      },
      "source": [
        "from __future__ import print_function \r\n",
        "from __future__ import division\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from PIL import Image\r\n",
        "import torchvision\r\n",
        "from torchvision import models, transforms\r\n",
        "import math\r\n",
        "import time\r\n",
        "import os\r\n",
        "import copy\r\n",
        "from stylegan_layers import  G_mapping,G_synthesis\r\n",
        "from collections import OrderedDict\r\n",
        "!pip install piq\r\n",
        "from piq import MultiScaleSSIMLoss\r\n",
        "from perceptual_model import VGG16_for_Perceptual\r\n",
        "from torchvision.utils import save_image\r\n",
        "\r\n",
        "print(\"PyTorch Version: \",torch.__version__)\r\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\r\n",
        "#torch.manual_seed(0)\r\n",
        "\r\n",
        "\r\n",
        "class FullyConnectedSparseLayer(nn.Module):\r\n",
        "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\r\n",
        "    def __init__(self, size_in, size_out, coef):\r\n",
        "        super().__init__()\r\n",
        "        self.size_in, self.size_out = size_in, size_out\r\n",
        "        weights = torch.Tensor(size_out, size_in)\r\n",
        "        self.weights = nn.Parameter(weights).to('cuda:0')  # nn.Parameter is a Tensor that's a module parameter.\r\n",
        "        inp_section = int(size_in/coef)\r\n",
        "        out_section = int(size_out/coef)\r\n",
        "        weight_canceler = [[0 for i in range(size_in)] for j in range(size_out)]     \r\n",
        "        count = 0\r\n",
        "        for i in range(size_out):\r\n",
        "            for j in range(count*inp_section,count*inp_section+inp_section):\r\n",
        "              weight_canceler[i][j] = 1\r\n",
        "            if (i+1)%out_section==0:\r\n",
        "              count = count + 1\r\n",
        "\r\n",
        "\r\n",
        "        self.weight_canceler = torch.Tensor(weight_canceler).to('cuda:0')\r\n",
        "        bias = torch.Tensor(size_out)\r\n",
        "        self.bias = nn.Parameter(bias).to('cuda:0')\r\n",
        "\r\n",
        "        # initialize weights and biases\r\n",
        "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\r\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\r\n",
        "        bound = 1 / math.sqrt(fan_in)\r\n",
        "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        weights = self.weights * self.weight_canceler\r\n",
        "        w_times_x= torch.mm(x, weights.t())\r\n",
        "        return torch.add(w_times_x, self.bias)  # w times x + b\r\n",
        "\r\n",
        "\r\n",
        "class SparseLayer(nn.Module):\r\n",
        "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\r\n",
        "    def __init__(self, size_in, size_out, steps):\r\n",
        "        super().__init__()\r\n",
        "        self.size_in, self.size_out = size_in, size_out\r\n",
        "        weights = torch.Tensor(size_out, size_in)\r\n",
        "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\r\n",
        "        try:\r\n",
        "          if int(steps):\r\n",
        "            steps = [steps]\r\n",
        "        except:\r\n",
        "          pass\r\n",
        "        weight_canceler = [[0 for i in range(size_in)] for j in range(size_out)]  \r\n",
        "        count = 0\r\n",
        "        \r\n",
        "        if len(steps) ==  2:\r\n",
        "          inp_el_counts = int(size_in/steps[0])\r\n",
        "          out_el_counts = int(size_out/steps[1])\r\n",
        "          steps_ratio = int(steps[0]/steps[1])\r\n",
        "          for i in range(steps[1]):\r\n",
        "              for j in range(i*out_el_counts,(i+1)*out_el_counts):\r\n",
        "                  for k in range((i+1)*steps_ratio):\r\n",
        "                      weight_canceler[j][k*inp_el_counts + count]=1\r\n",
        "                  count = count + 1\r\n",
        "                  if count >= inp_el_counts:\r\n",
        "                      count = 0\r\n",
        "        else:     \r\n",
        "          inp_el_counts = int(size_in/steps[0])   \r\n",
        "          for i in range(0,size_out):\r\n",
        "              for j in range(steps[0]):\r\n",
        "                  weight_canceler[i][j*inp_el_counts + count]=1\r\n",
        "              count = count + 1\r\n",
        "              if count >= inp_el_counts:\r\n",
        "                  count = 0\r\n",
        "        # groups_el_counts = int(size_in/step)\r\n",
        "        \r\n",
        "        # for i in range(0,size_out):\r\n",
        "        #     for j in range(step):\r\n",
        "        #         weight_canceler[i][j*groups_el_counts + count]=1\r\n",
        "        #     count = count + 1\r\n",
        "        #     if count >= groups_el_counts:\r\n",
        "        #         count = 0   \r\n",
        "\r\n",
        "\r\n",
        "        self.weight_canceler = torch.Tensor(weight_canceler).to('cuda:0')\r\n",
        "        bias = torch.Tensor(size_out)\r\n",
        "        self.bias = nn.Parameter(bias).to('cuda:0')\r\n",
        "\r\n",
        "        # initialize weights and biases\r\n",
        "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\r\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\r\n",
        "        bound = 1 / math.sqrt(fan_in)\r\n",
        "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        weights = self.weights * self.weight_canceler\r\n",
        "        w_times_x= torch.mm(x, weights.t())\r\n",
        "        return torch.add(w_times_x, self.bias)  # w times x + b\r\n",
        "\r\n",
        "\r\n",
        "class TreeConnect(nn.Module):\r\n",
        "    def __init__(self, input_dim, hidden_layers_dim , output_dim, div_coefs):\r\n",
        "        # percentage_masked, **kwargs\r\n",
        "        super(TreeConnect, self).__init__()\r\n",
        "        self.output_dim = output_dim\r\n",
        "        self.div_coefs = div_coefs \r\n",
        "        self.hidden_layers_dim = hidden_layers_dim\r\n",
        "        layers_parts = []\r\n",
        "        self.first_Layer = FullyConnectedSparseLayer(input_dim, hidden_layers_dim[0],div_coefs[0]).to('cuda:0')\r\n",
        "        #self.first_Layer = nn.Linear(input_dim, hidden_layers_dim[0])\r\n",
        "        hidden_layers = []\r\n",
        "        if len(hidden_layers_dim)>1:\r\n",
        "            for i in range(1,len(hidden_layers_dim)):\r\n",
        "                hidden_layers.append(SparseLayer(hidden_layers_dim[i-1], hidden_layers_dim[i], [div_coefs[i-1], div_coefs[i]]).to('cuda:0'))\r\n",
        "        self.hidden_layers = hidden_layers   \r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.last_layer = SparseLayer(hidden_layers_dim[-1], output_dim, div_coefs[-1]).to('cuda:0')\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        x = self.first_Layer(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        if len(self.hidden_layers)>0:\r\n",
        "            for hidden_layer in self.hidden_layers:\r\n",
        "                x = hidden_layer(x)\r\n",
        "                x = self.relu(x)\r\n",
        "        x = self.last_layer(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "class MyEnsemble(nn.Module):\r\n",
        "    def __init__(self, modelA, modelB, num_ftrs):\r\n",
        "        super(MyEnsemble, self).__init__()\r\n",
        "        self.modelA = modelA\r\n",
        "        self.modelB = modelB\r\n",
        "        # Remove last linear layer\r\n",
        "        self.modelA.fc = TreeConnect(input_dim=num_ftrs, hidden_layers_dim=[1024,1024],  output_dim=512, div_coefs=[64,32]).to('cuda:0')\r\n",
        "        self.modelB.fc = TreeConnect(input_dim=num_ftrs, hidden_layers_dim=[1024,1024],  output_dim=512, div_coefs=[64,32]).to('cuda:0')\r\n",
        "        #self.modelA.fc = nn.Sequential(\r\n",
        "        #    nn.Linear(in_features=num_ftrs, out_features=512),\r\n",
        "        #    nn.ReLU(),\r\n",
        "        #    # nn.Linear(in_features=1024, out_features=512),\r\n",
        "        #    # nn.ReLU(),\r\n",
        "        #    nn.Linear(512,512)\r\n",
        "        #)\r\n",
        "        #self.modelB.fc = nn.Sequential(\r\n",
        "        #    nn.Linear(in_features=num_ftrs, out_features=512),\r\n",
        "        #    nn.ReLU(),\r\n",
        "        #    # nn.Linear(in_features=1024, out_features=512),\r\n",
        "        #    # nn.ReLU(),\r\n",
        "        #    nn.Linear(512,512)\r\n",
        "        #) \r\n",
        "\r\n",
        "      \r\n",
        "    def forward(self, im1,im2):\r\n",
        "        x1 = self.modelA(im1.clone())\r\n",
        "        m1 = x1.detach()  # clone to make sure x is not changed by inplace methods\r\n",
        "        x2 = self.modelB(im2.clone())\r\n",
        "        m2 = x2.detach()\r\n",
        "        x = (x1 + x2) / 2.0\r\n",
        "        return x, m1, m2\r\n",
        "\r\n",
        "\r\n",
        "def image_preprocess(img_source):\r\n",
        "    img = Image.open(img_source).convert(\"RGB\")\r\n",
        "    img = transforms.ToTensor()(img).unsqueeze_(0)\r\n",
        "    #upsample2ds = torch.nn.Upsample(scale_factor=2, mode='bilinear')\r\n",
        "    #img = upsample2ds(img)\r\n",
        "    return img\r\n",
        "\r\n",
        "def adjust_lr(optimizer, lr):\r\n",
        "    for param in optimizer.param_groups:\r\n",
        "        param['lr'] = lr\r\n",
        "    return optimizer\r\n",
        "\r\n",
        "def caluclate_loss(synth_img, images, perceptual_net, img_p, upsample2d):\r\n",
        "    \r\n",
        "    synth_img_t = (synth_img - torch.min(synth_img))/(torch.max(synth_img)-torch.min(synth_img)).detach()\r\n",
        "    #synth_img_t = synth_img\r\n",
        "    ms_ssim_loss = MultiScaleSSIMLoss(data_range=1., reduction='none')(images[0], synth_img_t)\r\n",
        "    # ms_ssim_loss2 = MultiScaleSSIMLoss(data_range=1., reduction='none')(images[1], synth_img_t)\r\n",
        "    ms_ssim_loss = (ms_ssim_loss + MultiScaleSSIMLoss(data_range=1., reduction='none')(images[1], synth_img_t))/2.\r\n",
        "    MSE_Loss = nn.MSELoss(reduction=\"mean\")\r\n",
        "    #calculate Perceptual Loss\r\n",
        "    real1_0,real1_1,real1_2,real1_3=perceptual_net(img_p[0])\r\n",
        "    real2_0,real2_1,real2_2,real2_3=perceptual_net(img_p[1])\r\n",
        "    synth_p=upsample2d(synth_img) #(1,3,256,256)\r\n",
        "    synth_0,synth_1,synth_2,synth_3=perceptual_net(synth_p)\r\n",
        "\r\n",
        "    perceptual_loss=0\r\n",
        "    perceptual_loss = perceptual_loss + MSE_Loss(synth_0,real1_0) + MSE_Loss(synth_0,real2_0) \r\n",
        "    perceptual_loss = perceptual_loss + MSE_Loss(synth_1,real1_1) + MSE_Loss(synth_1,real2_1)\r\n",
        "    perceptual_loss = perceptual_loss + MSE_Loss(synth_2,real1_2) + MSE_Loss(synth_2,real2_2)\r\n",
        "    perceptual_loss = perceptual_loss + MSE_Loss(synth_3,real1_3) + MSE_Loss(synth_3,real2_3)\r\n",
        "\r\n",
        "    return ms_ssim_loss, perceptual_loss\r\n",
        "\r\n",
        "def identity_loss_calc(embedding1, embedding2):\r\n",
        "    morph = (embedding1 + embedding2)/2.0\r\n",
        "    identity_term1n = torch.mm(embedding1, torch.transpose(morph,0,1))\r\n",
        "    identity_term1d = torch.norm(embedding1) * torch.norm(morph)\r\n",
        "    identity_term2n = torch.mm(embedding2, torch.transpose(morph,0,1))\r\n",
        "    identity_term2d = torch.norm(embedding2) * torch.norm(morph)\r\n",
        "    identity_loss = ((1 - identity_term1n/identity_term1d) + (1 - identity_term2n/identity_term2d))/2.0\r\n",
        "    identity_diff = torch.abs(((1 - identity_term1n/identity_term1d) + (1 - identity_term2n/identity_term2d)))\r\n",
        "    return identity_loss, identity_diff\r\n",
        "\r\n",
        "def train_model(model, perceptual_net, g_synthesis, inputs, optimizer, lr, num_epochs, device):\r\n",
        "    global identity_loss\r\n",
        "    since = time.time()\r\n",
        "\r\n",
        "    #best_model1_wts = copy.deepcopy(model1.state_dict())\r\n",
        "    #best_model2_wts = copy.deepcopy(model2.state_dict())\r\n",
        "    image1 = inputs[0]\r\n",
        "    image2 = inputs[1]\r\n",
        "    loss_list = []\r\n",
        "\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\r\n",
        "        print('-' * 10)\r\n",
        "\r\n",
        "        # Each epoch has a training and validation phase\r\n",
        "        model.train()  # Set model to training mode\r\n",
        "        \r\n",
        "\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        morph, out1, out2 = model(image1, image2)\r\n",
        "        identity_loss, identity_diff = identity_loss_calc(out1, out2)\r\n",
        "\r\n",
        "        morph = morph.unsqueeze(1).repeat(1, 18, 1)\r\n",
        "        synth = g_synthesis(morph)\r\n",
        "        \r\n",
        "\r\n",
        "        img_p1=image1.clone() #Perceptual loss\r\n",
        "        img_p2=image2.clone()\r\n",
        "        upsample2d=torch.nn.Upsample(scale_factor=256/1024, mode='bilinear')\r\n",
        "        img_p1=upsample2d(img_p1)\r\n",
        "        img_p2=upsample2d(img_p2)\r\n",
        "        img_p = [img_p1, img_p2]\r\n",
        "        \r\n",
        "        ms_ssim_loss, perceptual_loss = caluclate_loss(synth, inputs, perceptual_net, img_p, upsample2d)\r\n",
        "\r\n",
        "        loss =  0.0002*perceptual_loss + ms_ssim_loss + 10*identity_loss.to(device) + identity_diff.to(device)\r\n",
        "\r\n",
        "        # backward + optimize only if in training phase\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        loss_np = loss.detach().cpu().numpy()\r\n",
        "        loss_p = perceptual_loss.detach().cpu().numpy()\r\n",
        "        loss_m = ms_ssim_loss.detach().cpu().numpy()\r\n",
        "        loss_idl = identity_loss.detach().cpu().numpy()\r\n",
        "        loss_idd = identity_diff.detach().cpu().numpy()\r\n",
        "        loss_list.append(loss_np)\r\n",
        "        if epoch%6==0:\r\n",
        "             lr = lr*0.95\r\n",
        "             optimizer = adjust_lr(optimizer, lr)\r\n",
        "\r\n",
        "        if epoch%10==0 or epoch==num_epochs-1:\r\n",
        "             print(\"iter{}: loss -- {},  ms_ssim --{},  percep_loss --{}, identity_loss --{}, identity_diff --{}\".format(epoch,loss_np,loss_m,loss_p,loss_idl,loss_idd))\r\n",
        "             synth = (synth - torch.min(synth))/(torch.max(synth)-torch.min(synth))\r\n",
        "             save_image(synth.clamp(0,1),\"save_image/morph/{}.png\".format(epoch))\r\n",
        "             #np.save(\"loss_list.npy\",loss_list)\r\n",
        "             \r\n",
        "\r\n",
        "    time_elapsed = time.time() - since\r\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n",
        "\r\n",
        "    # load best model weights\r\n",
        "    # model.load_state_dict(best_model_wts)\r\n",
        " \r\n",
        "# We use pretrained torchvision models here\r\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'   \r\n",
        "modelA = models.resnet50(pretrained=True).to(device)\r\n",
        "modelB = models.resnet50(pretrained=True).to(device)\r\n",
        "\r\n",
        "num_ftrs = modelA.fc.in_features\r\n",
        "\r\n",
        "# Freeze these models\r\n",
        "for param in modelA.parameters():\r\n",
        "    param.requires_grad_(False)\r\n",
        "\r\n",
        "for param in modelB.parameters():\r\n",
        "    param.requires_grad_(False)\r\n",
        "\r\n",
        "# Create ensemble model\r\n",
        "model = MyEnsemble(modelA, modelB,num_ftrs).to(device)\r\n",
        "\r\n",
        "img1 = image_preprocess(\"source_image/69959.png\").to(device)\r\n",
        "img2 = image_preprocess(\"source_image/69660.png\").to(device)\r\n",
        "inputs = [img1, img2]\r\n",
        "\r\n",
        "prms_to_update = []\r\n",
        "for name, param in model.named_parameters():\r\n",
        "    if param.requires_grad == True:\r\n",
        "        prms_to_update.append(param)\r\n",
        "\r\n",
        "g_synthesis = G_synthesis(resolution=1024)\r\n",
        "g_all = nn.Sequential(OrderedDict([\r\n",
        "        ('g_mapping', G_mapping()),\r\n",
        "        #('truncation', Truncation(avg_latent)),\r\n",
        "        ('g_synthesis', G_synthesis(resolution=1024))    \r\n",
        "        ]))\r\n",
        "\r\n",
        "g_all.load_state_dict(torch.load(\"weight_files/pytorch/karras2019stylegan-ffhq-1024x1024.for_g_all.pt\", map_location=device))\r\n",
        "g_all.eval()\r\n",
        "g_all.to(device)\r\n",
        "\r\n",
        "perceptual_net = VGG16_for_Perceptual(n_layers=[2,4,9,16]).to(device)\r\n",
        "\r\n",
        "g_synthesis = g_all[1]\r\n",
        "g_synthesis.eval()\r\n",
        "g_synthesis.to(device)\r\n",
        "del g_all\r\n",
        "torch.cuda.empty_cache()\r\n",
        "# Number of epochs to train for \r\n",
        "num_epochs = 150\r\n",
        "\r\n",
        "\r\n",
        "# Observe that all parameters are being optimized\r\n",
        "lr = 0.03\r\n",
        "optimizer = optim.Adam(prms_to_update, lr=lr, betas=(0.9,0.999))\r\n",
        "\r\n",
        "# Train and evaluate\r\n",
        "train_model(model, perceptual_net, g_synthesis, inputs, optimizer, lr, num_epochs=num_epochs, device=device)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting piq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/a2/c4ef48a8ed230ad1185de933ce466fb1204cba8cda6896a5c304a5e2db84/piq-0.5.4-py3-none-any.whl (102kB)\n",
            "\r\u001b[K     |███▏                            | 10kB 22.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 20kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 30kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 40kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 51kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 61kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 71kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 81kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 92kB 10.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 102kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from piq) (0.8.2+cu101)\n",
            "Collecting gudhi>=3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/71/e70015e0f547debe64901775202aa0e53231907a60850bb8d86ce8a31453/gudhi-3.4.1-cp37-cp37m-manylinux2014_x86_64.whl (28.1MB)\n",
            "\u001b[K     |████████████████████████████████| 28.1MB 112kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from piq) (1.4.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->piq) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->piq) (1.19.5)\n",
            "Requirement already satisfied: torch==1.7.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->piq) (1.7.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->torchvision>=0.4.0->piq) (3.7.4.3)\n",
            "Installing collected packages: gudhi, piq\n",
            "Successfully installed gudhi-3.4.1 piq-0.5.4\n",
            "PyTorch Version:  1.7.1+cu101\n",
            "Torchvision Version:  0.8.2+cu101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "405b5797cf9445d4accd2fe7db14a6c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1603d316f44e4f4baf5ca94a5d2b3e68",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 0/149\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter0: loss -- [[4.368363]],  ms_ssim --[0.588163],  percep_loss --16.683250427246094, identity_loss --[[0.3147386]], identity_diff --[[0.6294772]]\n",
            "Epoch 1/149\n",
            "----------\n",
            "Epoch 2/149\n",
            "----------\n",
            "Epoch 3/149\n",
            "----------\n",
            "Epoch 4/149\n",
            "----------\n",
            "Epoch 5/149\n",
            "----------\n",
            "Epoch 6/149\n",
            "----------\n",
            "Epoch 7/149\n",
            "----------\n",
            "Epoch 8/149\n",
            "----------\n",
            "Epoch 9/149\n",
            "----------\n",
            "Epoch 10/149\n",
            "----------\n",
            "iter10: loss -- [[0.97916085]],  ms_ssim --[0.45071027],  percep_loss --10.464151382446289, identity_loss --[[0.04386315]], identity_diff --[[0.08772629]]\n",
            "Epoch 11/149\n",
            "----------\n",
            "Epoch 12/149\n",
            "----------\n",
            "Epoch 13/149\n",
            "----------\n",
            "Epoch 14/149\n",
            "----------\n",
            "Epoch 15/149\n",
            "----------\n",
            "Epoch 16/149\n",
            "----------\n",
            "Epoch 17/149\n",
            "----------\n",
            "Epoch 18/149\n",
            "----------\n",
            "Epoch 19/149\n",
            "----------\n",
            "Epoch 20/149\n",
            "----------\n",
            "iter20: loss -- [[0.7260505]],  ms_ssim --[0.41853967],  percep_loss --8.906182289123535, identity_loss --[[0.02547747]], identity_diff --[[0.05095494]]\n",
            "Epoch 21/149\n",
            "----------\n",
            "Epoch 22/149\n",
            "----------\n",
            "Epoch 23/149\n",
            "----------\n",
            "Epoch 24/149\n",
            "----------\n",
            "Epoch 25/149\n",
            "----------\n",
            "Epoch 26/149\n",
            "----------\n",
            "Epoch 27/149\n",
            "----------\n",
            "Epoch 28/149\n",
            "----------\n",
            "Epoch 29/149\n",
            "----------\n",
            "Epoch 30/149\n",
            "----------\n",
            "iter30: loss -- [[0.65002227]],  ms_ssim --[0.40094736],  percep_loss --8.264572143554688, identity_loss --[[0.0206185]], identity_diff --[[0.041237]]\n",
            "Epoch 31/149\n",
            "----------\n",
            "Epoch 32/149\n",
            "----------\n",
            "Epoch 33/149\n",
            "----------\n",
            "Epoch 34/149\n",
            "----------\n",
            "Epoch 35/149\n",
            "----------\n",
            "Epoch 36/149\n",
            "----------\n",
            "Epoch 37/149\n",
            "----------\n",
            "Epoch 38/149\n",
            "----------\n",
            "Epoch 39/149\n",
            "----------\n",
            "Epoch 40/149\n",
            "----------\n",
            "iter40: loss -- [[0.61428654]],  ms_ssim --[0.3906739],  percep_loss --8.10427474975586, identity_loss --[[0.01849931]], identity_diff --[[0.03699863]]\n",
            "Epoch 41/149\n",
            "----------\n",
            "Epoch 42/149\n",
            "----------\n",
            "Epoch 43/149\n",
            "----------\n",
            "Epoch 44/149\n",
            "----------\n",
            "Epoch 45/149\n",
            "----------\n",
            "Epoch 46/149\n",
            "----------\n",
            "Epoch 47/149\n",
            "----------\n",
            "Epoch 48/149\n",
            "----------\n",
            "Epoch 49/149\n",
            "----------\n",
            "Epoch 50/149\n",
            "----------\n",
            "iter50: loss -- [[0.5919905]],  ms_ssim --[0.3843007],  percep_loss --8.001254081726074, identity_loss --[[0.01717412]], identity_diff --[[0.03434825]]\n",
            "Epoch 51/149\n",
            "----------\n",
            "Epoch 52/149\n",
            "----------\n",
            "Epoch 53/149\n",
            "----------\n",
            "Epoch 54/149\n",
            "----------\n",
            "Epoch 55/149\n",
            "----------\n",
            "Epoch 56/149\n",
            "----------\n",
            "Epoch 57/149\n",
            "----------\n",
            "Epoch 58/149\n",
            "----------\n",
            "Epoch 59/149\n",
            "----------\n",
            "Epoch 60/149\n",
            "----------\n",
            "iter60: loss -- [[0.58476496]],  ms_ssim --[0.38881695],  percep_loss --8.17951774597168, identity_loss --[[0.01619267]], identity_diff --[[0.03238535]]\n",
            "Epoch 61/149\n",
            "----------\n",
            "Epoch 62/149\n",
            "----------\n",
            "Epoch 63/149\n",
            "----------\n",
            "Epoch 64/149\n",
            "----------\n",
            "Epoch 65/149\n",
            "----------\n",
            "Epoch 66/149\n",
            "----------\n",
            "Epoch 67/149\n",
            "----------\n",
            "Epoch 68/149\n",
            "----------\n",
            "Epoch 69/149\n",
            "----------\n",
            "Epoch 70/149\n",
            "----------\n",
            "iter70: loss -- [[0.579192]],  ms_ssim --[0.39015335],  percep_loss --8.059598922729492, identity_loss --[[0.01561889]], identity_diff --[[0.03123778]]\n",
            "Epoch 71/149\n",
            "----------\n",
            "Epoch 72/149\n",
            "----------\n",
            "Epoch 73/149\n",
            "----------\n",
            "Epoch 74/149\n",
            "----------\n",
            "Epoch 75/149\n",
            "----------\n",
            "Epoch 76/149\n",
            "----------\n",
            "Epoch 77/149\n",
            "----------\n",
            "Epoch 78/149\n",
            "----------\n",
            "Epoch 79/149\n",
            "----------\n",
            "Epoch 80/149\n",
            "----------\n",
            "iter80: loss -- [[0.5681743]],  ms_ssim --[0.38295728],  percep_loss --7.753920555114746, identity_loss --[[0.01530552]], identity_diff --[[0.03061104]]\n",
            "Epoch 81/149\n",
            "----------\n",
            "Epoch 82/149\n",
            "----------\n",
            "Epoch 83/149\n",
            "----------\n",
            "Epoch 84/149\n",
            "----------\n",
            "Epoch 85/149\n",
            "----------\n",
            "Epoch 86/149\n",
            "----------\n",
            "Epoch 87/149\n",
            "----------\n",
            "Epoch 88/149\n",
            "----------\n",
            "Epoch 89/149\n",
            "----------\n",
            "Epoch 90/149\n",
            "----------\n",
            "iter90: loss -- [[0.563936]],  ms_ssim --[0.38169348],  percep_loss --7.889272689819336, identity_loss --[[0.01505539]], identity_diff --[[0.03011078]]\n",
            "Epoch 91/149\n",
            "----------\n",
            "Epoch 92/149\n",
            "----------\n",
            "Epoch 93/149\n",
            "----------\n",
            "Epoch 94/149\n",
            "----------\n",
            "Epoch 95/149\n",
            "----------\n",
            "Epoch 96/149\n",
            "----------\n",
            "Epoch 97/149\n",
            "----------\n",
            "Epoch 98/149\n",
            "----------\n",
            "Epoch 99/149\n",
            "----------\n",
            "Epoch 100/149\n",
            "----------\n",
            "iter100: loss -- [[0.5604211]],  ms_ssim --[0.38003156],  percep_loss --7.6617889404296875, identity_loss --[[0.01490477]], identity_diff --[[0.02980953]]\n",
            "Epoch 101/149\n",
            "----------\n",
            "Epoch 102/149\n",
            "----------\n",
            "Epoch 103/149\n",
            "----------\n",
            "Epoch 104/149\n",
            "----------\n",
            "Epoch 105/149\n",
            "----------\n",
            "Epoch 106/149\n",
            "----------\n",
            "Epoch 107/149\n",
            "----------\n",
            "Epoch 108/149\n",
            "----------\n",
            "Epoch 109/149\n",
            "----------\n",
            "Epoch 110/149\n",
            "----------\n",
            "iter110: loss -- [[0.55772626]],  ms_ssim --[0.37919953],  percep_loss --7.640349864959717, identity_loss --[[0.01474988]], identity_diff --[[0.02949977]]\n",
            "Epoch 111/149\n",
            "----------\n",
            "Epoch 112/149\n",
            "----------\n",
            "Epoch 113/149\n",
            "----------\n",
            "Epoch 114/149\n",
            "----------\n",
            "Epoch 115/149\n",
            "----------\n",
            "Epoch 116/149\n",
            "----------\n",
            "Epoch 117/149\n",
            "----------\n",
            "Epoch 118/149\n",
            "----------\n",
            "Epoch 119/149\n",
            "----------\n",
            "Epoch 120/149\n",
            "----------\n",
            "iter120: loss -- [[0.5553573]],  ms_ssim --[0.37843493],  percep_loss --7.8458099365234375, identity_loss --[[0.01461276]], identity_diff --[[0.02922553]]\n",
            "Epoch 121/149\n",
            "----------\n",
            "Epoch 122/149\n",
            "----------\n",
            "Epoch 123/149\n",
            "----------\n",
            "Epoch 124/149\n",
            "----------\n",
            "Epoch 125/149\n",
            "----------\n",
            "Epoch 126/149\n",
            "----------\n",
            "Epoch 127/149\n",
            "----------\n",
            "Epoch 128/149\n",
            "----------\n",
            "Epoch 129/149\n",
            "----------\n",
            "Epoch 130/149\n",
            "----------\n",
            "iter130: loss -- [[0.550688]],  ms_ssim --[0.37567735],  percep_loss --7.677021026611328, identity_loss --[[0.01445627]], identity_diff --[[0.02891254]]\n",
            "Epoch 131/149\n",
            "----------\n",
            "Epoch 132/149\n",
            "----------\n",
            "Epoch 133/149\n",
            "----------\n",
            "Epoch 134/149\n",
            "----------\n",
            "Epoch 135/149\n",
            "----------\n",
            "Epoch 136/149\n",
            "----------\n",
            "Epoch 137/149\n",
            "----------\n",
            "Epoch 138/149\n",
            "----------\n",
            "Epoch 139/149\n",
            "----------\n",
            "Epoch 140/149\n",
            "----------\n",
            "iter140: loss -- [[0.54843754]],  ms_ssim --[0.37517247],  percep_loss --7.6983442306518555, identity_loss --[[0.01431045]], identity_diff --[[0.0286209]]\n",
            "Epoch 141/149\n",
            "----------\n",
            "Epoch 142/149\n",
            "----------\n",
            "Epoch 143/149\n",
            "----------\n",
            "Epoch 144/149\n",
            "----------\n",
            "Epoch 145/149\n",
            "----------\n",
            "Epoch 146/149\n",
            "----------\n",
            "Epoch 147/149\n",
            "----------\n",
            "Epoch 148/149\n",
            "----------\n",
            "Epoch 149/149\n",
            "----------\n",
            "iter149: loss -- [[0.5479173]],  ms_ssim --[0.37593383],  percep_loss --7.616785526275635, identity_loss --[[0.01420501]], identity_diff --[[0.02841002]]\n",
            "Training complete in 1m 3s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b98-IFs7uHo4",
        "outputId": "1ba10110-51b7-4209-d751-9f9c6e7ec73b"
      },
      "source": [
        "!zip -r /content/file.zip /content/StyleGAN_LatentEditor/save_image/morph"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/ (stored 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/60.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/30.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/80.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/40.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/100.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/120.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/0.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/70.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/110.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/90.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/10.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/50.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/149.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/140.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/20.png (deflated 0%)\n",
            "  adding: content/StyleGAN_LatentEditor/save_image/morph/130.png (deflated 0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axso-SoZBLiV",
        "outputId": "203dab99-4b6a-4947-fc58-90a8c42c83a2"
      },
      "source": [
        "! pip install deepface\r\n",
        "from deepface import DeepFace\r\n",
        " \r\n",
        "#face verification\r\n",
        "obj = DeepFace.verify(\"149.png\", \"69660.png\", model_name = 'ArcFace')\r\n",
        "print(obj)\r\n",
        "#face verification\r\n",
        "obj = DeepFace.verify(\"149.png\", \"69959.png\", model_name = 'ArcFace')\r\n",
        "print(obj)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/1d/24c23d78aab3a22d0afe5ffa944116045f8c24e174dd718b32d3f136202a/deepface-0.0.49-py3-none-any.whl (55kB)\n",
            "\r\u001b[K     |█████▉                          | 10kB 20.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 20kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 30kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 40kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 51kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.7/dist-packages (from deepface) (1.1.5)\n",
            "Requirement already satisfied: opencv-python>=3.4.4 in /usr/local/lib/python3.7/dist-packages (from deepface) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from deepface) (1.19.5)\n",
            "Collecting gdown>=3.10.1\n",
            "  Downloading https://files.pythonhosted.org/packages/50/21/92c3cfe56f5c0647145c4b0083d0733dd4890a057eb100a8eeddf949ffe9/gdown-3.12.2.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mtcnn>=0.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/43/abee91792797c609c1bf30f1112117f7a87a713ebaa6ec5201d5555a73ef/mtcnn-0.1.0-py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from deepface) (7.0.0)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.7/dist-packages (from deepface) (4.41.1)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from deepface) (2.4.3)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from deepface) (2.4.1)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from deepface) (1.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->deepface) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->deepface) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown>=3.10.1->deepface) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown>=3.10.1->deepface) (3.0.12)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown>=3.10.1->deepface) (2.23.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.0->deepface) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.0->deepface) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.0->deepface) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (1.1.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (1.12.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (3.12.4)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (1.12)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (2.4.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (2.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (0.10.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (1.32.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->deepface) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->deepface) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->deepface) (2.11.3)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->deepface) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow>=1.9.0->deepface) (54.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.9.0->deepface) (0.4.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.9.0->deepface) (1.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.9.0->deepface) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.9.0->deepface) (1.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask>=1.1.2->deepface) (1.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=1.9.0->deepface) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.9.0->deepface) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.9.0->deepface) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.9.0->deepface) (4.2.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=1.9.0->deepface) (3.7.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=1.9.0->deepface) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.9.0->deepface) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=1.9.0->deepface) (3.4.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-cp37-none-any.whl size=9693 sha256=f8442cee0488f800fc390ebc4edba0437c6dee75db6453fda8c37b5d6c41b32a\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/d0/d7/d9983facc6f2775411803e0e2d30ebf98efbf2fc6e57701e09\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown, mtcnn, deepface\n",
            "  Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "Successfully installed deepface-0.0.49 gdown-3.12.2 mtcnn-0.1.0\n",
            "Directory  /root /.deepface created\n",
            "Directory  /root /.deepface/weights created\n",
            "arcface_weights.h5  will be downloaded to  /root/.deepface/weights/arcface_weights.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LVB3CdVejpmGHM28BpqqkbZP5hDEcdZY\n",
            "To: /root/.deepface/weights/arcface_weights.h5\n",
            "137MB [00:01, 88.9MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'verified': True, 'distance': 0.33886629343032837, 'max_threshold_to_verify': 0.6871912959056619, 'model': 'ArcFace', 'similarity_metric': 'cosine'}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}