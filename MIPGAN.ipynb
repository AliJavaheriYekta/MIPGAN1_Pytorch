{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MIPGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "512aced63424478ea49c604db0c27f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8f297b2913bf493791625558d6e93da6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_70e25fe733de42a9ae436e19a4e41572",
              "IPY_MODEL_d931981783ea47b3ac7b06c1460f3f92"
            ]
          }
        },
        "8f297b2913bf493791625558d6e93da6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70e25fe733de42a9ae436e19a4e41572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ea0d37ad10594612ac5bf690a2a2c31b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a7bbda65d96c4b9f8ce26e8305b2a9bf"
          }
        },
        "d931981783ea47b3ac7b06c1460f3f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_36f4e4488ce94865b1118b16d72fbd23",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:11&lt;00:00, 9.15MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61ce13d3de7f466bb1906e649d2b5e01"
          }
        },
        "ea0d37ad10594612ac5bf690a2a2c31b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a7bbda65d96c4b9f8ce26e8305b2a9bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36f4e4488ce94865b1118b16d72fbd23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61ce13d3de7f466bb1906e649d2b5e01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f88723854988483fa953255d3933010c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ec5c5b63c59c4769bf3c9ed9e113da0e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9a86b39ad5c540c99858730f9f203736",
              "IPY_MODEL_8fbcd475c9aa4c7abcc6f6b50be996d9"
            ]
          }
        },
        "ec5c5b63c59c4769bf3c9ed9e113da0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a86b39ad5c540c99858730f9f203736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f15fc7721f9a4c22983023fbce445794",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7ca05c9f65b4457918bba0f194ca30c"
          }
        },
        "8fbcd475c9aa4c7abcc6f6b50be996d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7ca8a1e45011437d99bcd690016b7c37",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:06&lt;00:00, 79.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0518fa3435944da79fa79e9aa1f45ce3"
          }
        },
        "f15fc7721f9a4c22983023fbce445794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7ca05c9f65b4457918bba0f194ca30c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ca8a1e45011437d99bcd690016b7c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0518fa3435944da79fa79e9aa1f45ce3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMH0_lGkpoqn"
      },
      "source": [
        "! git clone \"https://github.com/AliJavaheriYekta/MIPGAN1_Pytorch\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy-ncx32sB2x"
      },
      "source": [
        "%cd MIPGAN1_Pytorch/weights/\r\n",
        "! wget https://github.com/lernapparat/lernapparat/releases/download/v2019-02-01/karras2019stylegan-ffhq-1024x1024.for_g_all.pt\r\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "512aced63424478ea49c604db0c27f93",
            "8f297b2913bf493791625558d6e93da6",
            "70e25fe733de42a9ae436e19a4e41572",
            "d931981783ea47b3ac7b06c1460f3f92",
            "ea0d37ad10594612ac5bf690a2a2c31b",
            "a7bbda65d96c4b9f8ce26e8305b2a9bf",
            "36f4e4488ce94865b1118b16d72fbd23",
            "61ce13d3de7f466bb1906e649d2b5e01",
            "f88723854988483fa953255d3933010c",
            "ec5c5b63c59c4769bf3c9ed9e113da0e",
            "9a86b39ad5c540c99858730f9f203736",
            "8fbcd475c9aa4c7abcc6f6b50be996d9",
            "f15fc7721f9a4c22983023fbce445794",
            "d7ca05c9f65b4457918bba0f194ca30c",
            "7ca8a1e45011437d99bcd690016b7c37",
            "0518fa3435944da79fa79e9aa1f45ce3"
          ]
        },
        "id": "VD3_iZjtoKkp",
        "outputId": "d8339512-6f25-48b7-f93f-2b6f1a0a463f"
      },
      "source": [
        "from __future__ import print_function \r\n",
        "from __future__ import division\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from PIL import Image\r\n",
        "import torchvision\r\n",
        "from torchvision import models, transforms\r\n",
        "import math\r\n",
        "import time\r\n",
        "import os\r\n",
        "import copy\r\n",
        "from stylegan_layers import  G_mapping,G_synthesis\r\n",
        "from collections import OrderedDict\r\n",
        "!pip install piq\r\n",
        "from piq import MultiScaleSSIMLoss\r\n",
        "from perceptual_model import VGG16_for_Perceptual\r\n",
        "from torchvision.utils import save_image\r\n",
        "\r\n",
        "print(\"PyTorch Version: \",torch.__version__)\r\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\r\n",
        "#torch.manual_seed(0)\r\n",
        "\r\n",
        "\r\n",
        "class FullyConnectedSparseLayer(nn.Module):\r\n",
        "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\r\n",
        "    def __init__(self, size_in, size_out, coef):\r\n",
        "        super().__init__()\r\n",
        "        self.size_in, self.size_out = size_in, size_out\r\n",
        "        weights = torch.Tensor(size_out, size_in)\r\n",
        "        self.weights = nn.Parameter(weights).to('cuda:0')  # nn.Parameter is a Tensor that's a module parameter.\r\n",
        "        inp_section = int(size_in/coef)\r\n",
        "        out_section = int(size_out/coef)\r\n",
        "        weight_canceler = [[0 for i in range(size_in)] for j in range(size_out)]     \r\n",
        "        count = 0\r\n",
        "        for i in range(size_out):\r\n",
        "            for j in range(count*inp_section,count*inp_section+inp_section):\r\n",
        "              weight_canceler[i][j] = 1\r\n",
        "            if (i+1)%out_section==0:\r\n",
        "              count = count + 1\r\n",
        "\r\n",
        "\r\n",
        "        self.weight_canceler = torch.Tensor(weight_canceler).to('cuda:0')\r\n",
        "        bias = torch.Tensor(size_out)\r\n",
        "        self.bias = nn.Parameter(bias).to('cuda:0')\r\n",
        "\r\n",
        "        # initialize weights and biases\r\n",
        "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\r\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\r\n",
        "        bound = 1 / math.sqrt(fan_in)\r\n",
        "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        weights = self.weights * self.weight_canceler\r\n",
        "        w_times_x= torch.mm(x, weights.t())\r\n",
        "        return torch.add(w_times_x, self.bias)  # w times x + b\r\n",
        "\r\n",
        "\r\n",
        "class SparseLayer(nn.Module):\r\n",
        "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\r\n",
        "    def __init__(self, size_in, size_out, steps):\r\n",
        "        super().__init__()\r\n",
        "        self.size_in, self.size_out = size_in, size_out\r\n",
        "        weights = torch.Tensor(size_out, size_in)\r\n",
        "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\r\n",
        "        try:\r\n",
        "          if int(steps):\r\n",
        "            steps = [steps]\r\n",
        "        except:\r\n",
        "          pass\r\n",
        "        weight_canceler = [[0 for i in range(size_in)] for j in range(size_out)]  \r\n",
        "        count = 0\r\n",
        "        \r\n",
        "        if len(steps) ==  2:\r\n",
        "          inp_el_counts = int(size_in/steps[0])\r\n",
        "          out_el_counts = int(size_out/steps[1])\r\n",
        "          steps_ratio = int(steps[0]/steps[1])\r\n",
        "          for i in range(steps[1]):\r\n",
        "              for j in range(i*out_el_counts,(i+1)*out_el_counts):\r\n",
        "                  for k in range((i+1)*steps_ratio):\r\n",
        "                      weight_canceler[j][k*inp_el_counts + count]=1\r\n",
        "                  count = count + 1\r\n",
        "                  if count >= inp_el_counts:\r\n",
        "                      count = 0\r\n",
        "        else:     \r\n",
        "          inp_el_counts = int(size_in/steps[0])   \r\n",
        "          for i in range(0,size_out):\r\n",
        "              for j in range(steps[0]):\r\n",
        "                  weight_canceler[i][j*inp_el_counts + count]=1\r\n",
        "              count = count + 1\r\n",
        "              if count >= inp_el_counts:\r\n",
        "                  count = 0\r\n",
        "        # groups_el_counts = int(size_in/step)\r\n",
        "        \r\n",
        "        # for i in range(0,size_out):\r\n",
        "        #     for j in range(step):\r\n",
        "        #         weight_canceler[i][j*groups_el_counts + count]=1\r\n",
        "        #     count = count + 1\r\n",
        "        #     if count >= groups_el_counts:\r\n",
        "        #         count = 0   \r\n",
        "\r\n",
        "\r\n",
        "        self.weight_canceler = torch.Tensor(weight_canceler).to('cuda:0')\r\n",
        "        bias = torch.Tensor(size_out)\r\n",
        "        self.bias = nn.Parameter(bias).to('cuda:0')\r\n",
        "\r\n",
        "        # initialize weights and biases\r\n",
        "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\r\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\r\n",
        "        bound = 1 / math.sqrt(fan_in)\r\n",
        "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        weights = self.weights * self.weight_canceler\r\n",
        "        w_times_x= torch.mm(x, weights.t())\r\n",
        "        return torch.add(w_times_x, self.bias)  # w times x + b\r\n",
        "\r\n",
        "\r\n",
        "class TreeConnect(nn.Module):\r\n",
        "    def __init__(self, input_dim, hidden_layers_dim , output_dim, div_coefs):\r\n",
        "        # percentage_masked, **kwargs\r\n",
        "        super(TreeConnect, self).__init__()\r\n",
        "        self.output_dim = output_dim\r\n",
        "        self.div_coefs = div_coefs \r\n",
        "        self.hidden_layers_dim = hidden_layers_dim\r\n",
        "        layers_parts = []\r\n",
        "        self.first_Layer = FullyConnectedSparseLayer(input_dim, hidden_layers_dim[0],div_coefs[0]).to('cuda:0')\r\n",
        "        #self.first_Layer = nn.Linear(input_dim, hidden_layers_dim[0])\r\n",
        "        hidden_layers = []\r\n",
        "        if len(hidden_layers_dim)>1:\r\n",
        "            for i in range(1,len(hidden_layers_dim)):\r\n",
        "                hidden_layers.append(SparseLayer(hidden_layers_dim[i-1], hidden_layers_dim[i], [div_coefs[i-1], div_coefs[i]]).to('cuda:0'))\r\n",
        "        self.hidden_layers = hidden_layers   \r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.last_layer = SparseLayer(hidden_layers_dim[-1], output_dim, div_coefs[-1]).to('cuda:0')\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        x = self.first_Layer(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        if len(self.hidden_layers)>0:\r\n",
        "            for hidden_layer in self.hidden_layers:\r\n",
        "                x = hidden_layer(x)\r\n",
        "                x = self.relu(x)\r\n",
        "        x = self.last_layer(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "class MyEnsemble(nn.Module):\r\n",
        "    def __init__(self, modelA, modelB, num_ftrs):\r\n",
        "        super(MyEnsemble, self).__init__()\r\n",
        "        self.modelA = modelA\r\n",
        "        self.modelB = modelB\r\n",
        "        # Remove last linear layer\r\n",
        "        self.modelA.fc = TreeConnect(input_dim=num_ftrs, hidden_layers_dim=[1024,1024],  output_dim=512, div_coefs=[64,32]).to('cuda:0')\r\n",
        "        self.modelB.fc = TreeConnect(input_dim=num_ftrs, hidden_layers_dim=[1024,1024],  output_dim=512, div_coefs=[64,32]).to('cuda:0')\r\n",
        "        #self.modelA.fc = nn.Sequential(\r\n",
        "        #    nn.Linear(in_features=num_ftrs, out_features=512),\r\n",
        "        #    nn.ReLU(),\r\n",
        "        #    # nn.Linear(in_features=1024, out_features=512),\r\n",
        "        #    # nn.ReLU(),\r\n",
        "        #    nn.Linear(512,512)\r\n",
        "        #)\r\n",
        "        #self.modelB.fc = nn.Sequential(\r\n",
        "        #    nn.Linear(in_features=num_ftrs, out_features=512),\r\n",
        "        #    nn.ReLU(),\r\n",
        "        #    # nn.Linear(in_features=1024, out_features=512),\r\n",
        "        #    # nn.ReLU(),\r\n",
        "        #    nn.Linear(512,512)\r\n",
        "        #) \r\n",
        "\r\n",
        "      \r\n",
        "    def forward(self, im1,im2):\r\n",
        "        x1 = self.modelA(im1.clone())\r\n",
        "        m1 = x1.detach()  # clone to make sure x is not changed by inplace methods\r\n",
        "        x2 = self.modelB(im2.clone())\r\n",
        "        m2 = x2.detach()\r\n",
        "        x = (x1 + x2) / 2.0\r\n",
        "        return x, m1, m2\r\n",
        "\r\n",
        "\r\n",
        "def image_preprocess(img_source):\r\n",
        "    img = Image.open(img_source).convert(\"RGB\")\r\n",
        "    img = transforms.ToTensor()(img).unsqueeze_(0)\r\n",
        "    #upsample2ds = torch.nn.Upsample(scale_factor=2, mode='bilinear')\r\n",
        "    #img = upsample2ds(img)\r\n",
        "    return img\r\n",
        "\r\n",
        "def adjust_lr(optimizer, lr):\r\n",
        "    for param in optimizer.param_groups:\r\n",
        "        param['lr'] = lr\r\n",
        "    return optimizer\r\n",
        "\r\n",
        "def caluclate_loss(synth_img, images, perceptual_net, img_p, upsample2d):\r\n",
        "    \r\n",
        "    synth_img_t = (synth_img - torch.min(synth_img))/(torch.max(synth_img)-torch.min(synth_img)).detach()\r\n",
        "    #synth_img_t = synth_img\r\n",
        "    ms_ssim_loss = MultiScaleSSIMLoss(data_range=1., reduction='none')(images[0], synth_img_t)\r\n",
        "    # ms_ssim_loss2 = MultiScaleSSIMLoss(data_range=1., reduction='none')(images[1], synth_img_t)\r\n",
        "    ms_ssim_loss = (ms_ssim_loss + MultiScaleSSIMLoss(data_range=1., reduction='none')(images[1], synth_img_t))/2.\r\n",
        "    MSE_Loss = nn.MSELoss(reduction=\"mean\")\r\n",
        "    #calculate Perceptual Loss\r\n",
        "    real1_0,real1_1,real1_2,real1_3=perceptual_net(img_p[0])\r\n",
        "    real2_0,real2_1,real2_2,real2_3=perceptual_net(img_p[1])\r\n",
        "    synth_p=upsample2d(synth_img) #(1,3,256,256)\r\n",
        "    synth_0,synth_1,synth_2,synth_3=perceptual_net(synth_p)\r\n",
        "\r\n",
        "    perceptual_loss=0\r\n",
        "    perceptual_loss = perceptual_loss + MSE_Loss(synth_0,real1_0) + MSE_Loss(synth_0,real2_0) \r\n",
        "    perceptual_loss = perceptual_loss + MSE_Loss(synth_1,real1_1) + MSE_Loss(synth_1,real2_1)\r\n",
        "    perceptual_loss = perceptual_loss + MSE_Loss(synth_2,real1_2) + MSE_Loss(synth_2,real2_2)\r\n",
        "    perceptual_loss = perceptual_loss + MSE_Loss(synth_3,real1_3) + MSE_Loss(synth_3,real2_3)\r\n",
        "\r\n",
        "    return ms_ssim_loss, perceptual_loss\r\n",
        "\r\n",
        "def identity_loss_calc(embedding1, embedding2):\r\n",
        "    morph = (embedding1 + embedding2)/2.0\r\n",
        "    identity_term1n = torch.mm(embedding1, torch.transpose(morph,0,1))\r\n",
        "    identity_term1d = torch.norm(embedding1) * torch.norm(morph)\r\n",
        "    identity_term2n = torch.mm(embedding2, torch.transpose(morph,0,1))\r\n",
        "    identity_term2d = torch.norm(embedding2) * torch.norm(morph)\r\n",
        "    identity_loss = ((1 - identity_term1n/identity_term1d) + (1 - identity_term2n/identity_term2d))/2.0\r\n",
        "    identity_diff = torch.abs(((1 - identity_term1n/identity_term1d) + (1 - identity_term2n/identity_term2d)))\r\n",
        "    return identity_loss, identity_diff\r\n",
        "\r\n",
        "def train_model(model, perceptual_net, g_synthesis, inputs, optimizer, lr, num_epochs, device):\r\n",
        "    global identity_loss\r\n",
        "    since = time.time()\r\n",
        "\r\n",
        "    #best_model1_wts = copy.deepcopy(model1.state_dict())\r\n",
        "    #best_model2_wts = copy.deepcopy(model2.state_dict())\r\n",
        "    image1 = inputs[0]\r\n",
        "    image2 = inputs[1]\r\n",
        "    loss_list = []\r\n",
        "\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\r\n",
        "        print('-' * 10)\r\n",
        "\r\n",
        "        # Each epoch has a training and validation phase\r\n",
        "        model.train()  # Set model to training mode\r\n",
        "        \r\n",
        "\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        morph, out1, out2 = model(image1, image2)\r\n",
        "        identity_loss, identity_diff = identity_loss_calc(out1, out2)\r\n",
        "\r\n",
        "        morph = morph.unsqueeze(1).repeat(1, 18, 1)\r\n",
        "        synth = g_synthesis(morph)\r\n",
        "        \r\n",
        "\r\n",
        "        img_p1=image1.clone() #Perceptual loss\r\n",
        "        img_p2=image2.clone()\r\n",
        "        upsample2d=torch.nn.Upsample(scale_factor=256/1024, mode='bilinear')\r\n",
        "        img_p1=upsample2d(img_p1)\r\n",
        "        img_p2=upsample2d(img_p2)\r\n",
        "        img_p = [img_p1, img_p2]\r\n",
        "        \r\n",
        "        ms_ssim_loss, perceptual_loss = caluclate_loss(synth, inputs, perceptual_net, img_p, upsample2d)\r\n",
        "\r\n",
        "        loss =  0.0002*perceptual_loss + ms_ssim_loss + 10*identity_loss.to(device) + identity_diff.to(device)\r\n",
        "\r\n",
        "        # backward + optimize only if in training phase\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        loss_np = loss.detach().cpu().numpy()\r\n",
        "        loss_p = perceptual_loss.detach().cpu().numpy()\r\n",
        "        loss_m = ms_ssim_loss.detach().cpu().numpy()\r\n",
        "        loss_idl = identity_loss.detach().cpu().numpy()\r\n",
        "        loss_idd = identity_diff.detach().cpu().numpy()\r\n",
        "        loss_list.append(loss_np)\r\n",
        "        if epoch%6==0:\r\n",
        "             lr = lr*0.95\r\n",
        "             optimizer = adjust_lr(optimizer, lr)\r\n",
        "\r\n",
        "        if epoch%10==0 or epoch==num_epochs-1:\r\n",
        "             print(\"iter{}: loss -- {},  ms_ssim --{},  percep_loss --{}, identity_loss --{}, identity_diff --{}\".format(epoch,loss_np,loss_m,loss_p,loss_idl,loss_idd))\r\n",
        "             synth = (synth - torch.min(synth))/(torch.max(synth)-torch.min(synth))\r\n",
        "             save_image(synth.clamp(0,1),\"save_result/{}.png\".format(epoch))\r\n",
        "             #np.save(\"loss_list.npy\",loss_list)\r\n",
        "             \r\n",
        "\r\n",
        "    time_elapsed = time.time() - since\r\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n",
        "\r\n",
        "    # load best model weights\r\n",
        "    # model.load_state_dict(best_model_wts)\r\n",
        " \r\n",
        "# We use pretrained torchvision models here\r\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'   \r\n",
        "modelA = models.resnet50(pretrained=True).to(device)\r\n",
        "modelB = models.resnet50(pretrained=True).to(device)\r\n",
        "\r\n",
        "num_ftrs = modelA.fc.in_features\r\n",
        "\r\n",
        "# Freeze these models\r\n",
        "for param in modelA.parameters():\r\n",
        "    param.requires_grad_(False)\r\n",
        "\r\n",
        "for param in modelB.parameters():\r\n",
        "    param.requires_grad_(False)\r\n",
        "\r\n",
        "# Create ensemble model\r\n",
        "model = MyEnsemble(modelA, modelB,num_ftrs).to(device)\r\n",
        "\r\n",
        "img1 = image_preprocess(\"source_image/3.png\").to(device)\r\n",
        "img2 = image_preprocess(\"source_image/4.png\").to(device)\r\n",
        "inputs = [img1, img2]\r\n",
        "\r\n",
        "prms_to_update = []\r\n",
        "for name, param in model.named_parameters():\r\n",
        "    if param.requires_grad == True:\r\n",
        "        prms_to_update.append(param)\r\n",
        "\r\n",
        "g_synthesis = G_synthesis(resolution=1024)\r\n",
        "g_all = nn.Sequential(OrderedDict([\r\n",
        "        ('g_mapping', G_mapping()),\r\n",
        "        #('truncation', Truncation(avg_latent)),\r\n",
        "        ('g_synthesis', G_synthesis(resolution=1024))    \r\n",
        "        ]))\r\n",
        "\r\n",
        "g_all.load_state_dict(torch.load(\"weights/karras2019stylegan-ffhq-1024x1024.for_g_all.pt\", map_location=device))\r\n",
        "g_all.eval()\r\n",
        "g_all.to(device)\r\n",
        "\r\n",
        "perceptual_net = VGG16_for_Perceptual(n_layers=[2,4,9,16]).to(device)\r\n",
        "\r\n",
        "g_synthesis = g_all[1]\r\n",
        "g_synthesis.eval()\r\n",
        "g_synthesis.to(device)\r\n",
        "del g_all\r\n",
        "torch.cuda.empty_cache()\r\n",
        "# Number of epochs to train for \r\n",
        "num_epochs = 150\r\n",
        "\r\n",
        "\r\n",
        "# Observe that all parameters are being optimized\r\n",
        "lr = 0.03\r\n",
        "optimizer = optim.Adam(prms_to_update, lr=lr, betas=(0.9,0.999))\r\n",
        "\r\n",
        "# Train and evaluate\r\n",
        "train_model(model, perceptual_net, g_synthesis, inputs, optimizer, lr, num_epochs=num_epochs, device=device)\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting piq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/a2/c4ef48a8ed230ad1185de933ce466fb1204cba8cda6896a5c304a5e2db84/piq-0.5.4-py3-none-any.whl (102kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 10.9MB/s \n",
            "\u001b[?25hCollecting gudhi>=3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/71/e70015e0f547debe64901775202aa0e53231907a60850bb8d86ce8a31453/gudhi-3.4.1-cp37-cp37m-manylinux2014_x86_64.whl (28.1MB)\n",
            "\u001b[K     |████████████████████████████████| 28.1MB 101kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from piq) (1.4.1)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from piq) (0.9.0+cu101)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.7/dist-packages (from gudhi>=3.2->piq) (1.19.5)\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->piq) (1.8.0+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->piq) (7.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->torchvision>=0.4.0->piq) (3.7.4.3)\n",
            "Installing collected packages: gudhi, piq\n",
            "Successfully installed gudhi-3.4.1 piq-0.5.4\n",
            "PyTorch Version:  1.8.0+cu101\n",
            "Torchvision Version:  0.9.0+cu101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "512aced63424478ea49c604db0c27f93",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f88723854988483fa953255d3933010c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 0/149\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3455: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3500: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter0: loss -- [[4.471236]],  ms_ssim --[0.5929652],  percep_loss --16.61325454711914, identity_loss --[[0.32291234]], identity_diff --[[0.6458247]]\n",
            "Epoch 1/149\n",
            "----------\n",
            "Epoch 2/149\n",
            "----------\n",
            "Epoch 3/149\n",
            "----------\n",
            "Epoch 4/149\n",
            "----------\n",
            "Epoch 5/149\n",
            "----------\n",
            "Epoch 6/149\n",
            "----------\n",
            "Epoch 7/149\n",
            "----------\n",
            "Epoch 8/149\n",
            "----------\n",
            "Epoch 9/149\n",
            "----------\n",
            "Epoch 10/149\n",
            "----------\n",
            "iter10: loss -- [[1.4309639]],  ms_ssim --[0.45728964],  percep_loss --11.430696487426758, identity_loss --[[0.08094901]], identity_diff --[[0.16189802]]\n",
            "Epoch 11/149\n",
            "----------\n",
            "Epoch 12/149\n",
            "----------\n",
            "Epoch 13/149\n",
            "----------\n",
            "Epoch 14/149\n",
            "----------\n",
            "Epoch 15/149\n",
            "----------\n",
            "Epoch 16/149\n",
            "----------\n",
            "Epoch 17/149\n",
            "----------\n",
            "Epoch 18/149\n",
            "----------\n",
            "Epoch 19/149\n",
            "----------\n",
            "Epoch 20/149\n",
            "----------\n",
            "iter20: loss -- [[0.9556558]],  ms_ssim --[0.40384266],  percep_loss --8.478487968444824, identity_loss --[[0.04584312]], identity_diff --[[0.09168625]]\n",
            "Epoch 21/149\n",
            "----------\n",
            "Epoch 22/149\n",
            "----------\n",
            "Epoch 23/149\n",
            "----------\n",
            "Epoch 24/149\n",
            "----------\n",
            "Epoch 25/149\n",
            "----------\n",
            "Epoch 26/149\n",
            "----------\n",
            "Epoch 27/149\n",
            "----------\n",
            "Epoch 28/149\n",
            "----------\n",
            "Epoch 29/149\n",
            "----------\n",
            "Epoch 30/149\n",
            "----------\n",
            "iter30: loss -- [[0.82059747]],  ms_ssim --[0.374134],  percep_loss --7.312094688415527, identity_loss --[[0.03708342]], identity_diff --[[0.07416683]]\n",
            "Epoch 31/149\n",
            "----------\n",
            "Epoch 32/149\n",
            "----------\n",
            "Epoch 33/149\n",
            "----------\n",
            "Epoch 34/149\n",
            "----------\n",
            "Epoch 35/149\n",
            "----------\n",
            "Epoch 36/149\n",
            "----------\n",
            "Epoch 37/149\n",
            "----------\n",
            "Epoch 38/149\n",
            "----------\n",
            "Epoch 39/149\n",
            "----------\n",
            "Epoch 40/149\n",
            "----------\n",
            "iter40: loss -- [[0.7662118]],  ms_ssim --[0.3594137],  percep_loss --7.057852745056152, identity_loss --[[0.03378221]], identity_diff --[[0.06756443]]\n",
            "Epoch 41/149\n",
            "----------\n",
            "Epoch 42/149\n",
            "----------\n",
            "Epoch 43/149\n",
            "----------\n",
            "Epoch 44/149\n",
            "----------\n",
            "Epoch 45/149\n",
            "----------\n",
            "Epoch 46/149\n",
            "----------\n",
            "Epoch 47/149\n",
            "----------\n",
            "Epoch 48/149\n",
            "----------\n",
            "Epoch 49/149\n",
            "----------\n",
            "Epoch 50/149\n",
            "----------\n",
            "iter50: loss -- [[0.73812896]],  ms_ssim --[0.34868908],  percep_loss --6.780416488647461, identity_loss --[[0.03234032]], identity_diff --[[0.06468064]]\n",
            "Epoch 51/149\n",
            "----------\n",
            "Epoch 52/149\n",
            "----------\n",
            "Epoch 53/149\n",
            "----------\n",
            "Epoch 54/149\n",
            "----------\n",
            "Epoch 55/149\n",
            "----------\n",
            "Epoch 56/149\n",
            "----------\n",
            "Epoch 57/149\n",
            "----------\n",
            "Epoch 58/149\n",
            "----------\n",
            "Epoch 59/149\n",
            "----------\n",
            "Epoch 60/149\n",
            "----------\n",
            "iter60: loss -- [[0.7053382]],  ms_ssim --[0.32819808],  percep_loss --6.373298168182373, identity_loss --[[0.03132212]], identity_diff --[[0.06264424]]\n",
            "Epoch 61/149\n",
            "----------\n",
            "Epoch 62/149\n",
            "----------\n",
            "Epoch 63/149\n",
            "----------\n",
            "Epoch 64/149\n",
            "----------\n",
            "Epoch 65/149\n",
            "----------\n",
            "Epoch 66/149\n",
            "----------\n",
            "Epoch 67/149\n",
            "----------\n",
            "Epoch 68/149\n",
            "----------\n",
            "Epoch 69/149\n",
            "----------\n",
            "Epoch 70/149\n",
            "----------\n",
            "iter70: loss -- [[0.69297874]],  ms_ssim --[0.32470992],  percep_loss --6.459047317504883, identity_loss --[[0.03058141]], identity_diff --[[0.06116283]]\n",
            "Epoch 71/149\n",
            "----------\n",
            "Epoch 72/149\n",
            "----------\n",
            "Epoch 73/149\n",
            "----------\n",
            "Epoch 74/149\n",
            "----------\n",
            "Epoch 75/149\n",
            "----------\n",
            "Epoch 76/149\n",
            "----------\n",
            "Epoch 77/149\n",
            "----------\n",
            "Epoch 78/149\n",
            "----------\n",
            "Epoch 79/149\n",
            "----------\n",
            "Epoch 80/149\n",
            "----------\n",
            "iter80: loss -- [[0.67586446]],  ms_ssim --[0.31493717],  percep_loss --6.354959487915039, identity_loss --[[0.02997136]], identity_diff --[[0.05994272]]\n",
            "Epoch 81/149\n",
            "----------\n",
            "Epoch 82/149\n",
            "----------\n",
            "Epoch 83/149\n",
            "----------\n",
            "Epoch 84/149\n",
            "----------\n",
            "Epoch 85/149\n",
            "----------\n",
            "Epoch 86/149\n",
            "----------\n",
            "Epoch 87/149\n",
            "----------\n",
            "Epoch 88/149\n",
            "----------\n",
            "Epoch 89/149\n",
            "----------\n",
            "Epoch 90/149\n",
            "----------\n",
            "iter90: loss -- [[0.6663156]],  ms_ssim --[0.31151512],  percep_loss --6.414053440093994, identity_loss --[[0.0294598]], identity_diff --[[0.05891961]]\n",
            "Epoch 91/149\n",
            "----------\n",
            "Epoch 92/149\n",
            "----------\n",
            "Epoch 93/149\n",
            "----------\n",
            "Epoch 94/149\n",
            "----------\n",
            "Epoch 95/149\n",
            "----------\n",
            "Epoch 96/149\n",
            "----------\n",
            "Epoch 97/149\n",
            "----------\n",
            "Epoch 98/149\n",
            "----------\n",
            "Epoch 99/149\n",
            "----------\n",
            "Epoch 100/149\n",
            "----------\n",
            "iter100: loss -- [[0.65671027]],  ms_ssim --[0.30841106],  percep_loss --6.292724609375, identity_loss --[[0.02892005]], identity_diff --[[0.05784011]]\n",
            "Epoch 101/149\n",
            "----------\n",
            "Epoch 102/149\n",
            "----------\n",
            "Epoch 103/149\n",
            "----------\n",
            "Epoch 104/149\n",
            "----------\n",
            "Epoch 105/149\n",
            "----------\n",
            "Epoch 106/149\n",
            "----------\n",
            "Epoch 107/149\n",
            "----------\n",
            "Epoch 108/149\n",
            "----------\n",
            "Epoch 109/149\n",
            "----------\n",
            "Epoch 110/149\n",
            "----------\n",
            "iter110: loss -- [[0.6483527]],  ms_ssim --[0.3052682],  percep_loss --6.367201805114746, identity_loss --[[0.02848426]], identity_diff --[[0.05696851]]\n",
            "Epoch 111/149\n",
            "----------\n",
            "Epoch 112/149\n",
            "----------\n",
            "Epoch 113/149\n",
            "----------\n",
            "Epoch 114/149\n",
            "----------\n",
            "Epoch 115/149\n",
            "----------\n",
            "Epoch 116/149\n",
            "----------\n",
            "Epoch 117/149\n",
            "----------\n",
            "Epoch 118/149\n",
            "----------\n",
            "Epoch 119/149\n",
            "----------\n",
            "Epoch 120/149\n",
            "----------\n",
            "iter120: loss -- [[0.6455035]],  ms_ssim --[0.30690804],  percep_loss --6.311213493347168, identity_loss --[[0.0281111]], identity_diff --[[0.0562222]]\n",
            "Epoch 121/149\n",
            "----------\n",
            "Epoch 122/149\n",
            "----------\n",
            "Epoch 123/149\n",
            "----------\n",
            "Epoch 124/149\n",
            "----------\n",
            "Epoch 125/149\n",
            "----------\n",
            "Epoch 126/149\n",
            "----------\n",
            "Epoch 127/149\n",
            "----------\n",
            "Epoch 128/149\n",
            "----------\n",
            "Epoch 129/149\n",
            "----------\n",
            "Epoch 130/149\n",
            "----------\n",
            "iter130: loss -- [[0.6332757]],  ms_ssim --[0.29883206],  percep_loss --6.135279655456543, identity_loss --[[0.02776805]], identity_diff --[[0.05553609]]\n",
            "Epoch 131/149\n",
            "----------\n",
            "Epoch 132/149\n",
            "----------\n",
            "Epoch 133/149\n",
            "----------\n",
            "Epoch 134/149\n",
            "----------\n",
            "Epoch 135/149\n",
            "----------\n",
            "Epoch 136/149\n",
            "----------\n",
            "Epoch 137/149\n",
            "----------\n",
            "Epoch 138/149\n",
            "----------\n",
            "Epoch 139/149\n",
            "----------\n",
            "Epoch 140/149\n",
            "----------\n",
            "iter140: loss -- [[0.63033766]],  ms_ssim --[0.29977396],  percep_loss --6.006538391113281, identity_loss --[[0.02744687]], identity_diff --[[0.05489373]]\n",
            "Epoch 141/149\n",
            "----------\n",
            "Epoch 142/149\n",
            "----------\n",
            "Epoch 143/149\n",
            "----------\n",
            "Epoch 144/149\n",
            "----------\n",
            "Epoch 145/149\n",
            "----------\n",
            "Epoch 146/149\n",
            "----------\n",
            "Epoch 147/149\n",
            "----------\n",
            "Epoch 148/149\n",
            "----------\n",
            "Epoch 149/149\n",
            "----------\n",
            "iter149: loss -- [[0.6358981]],  ms_ssim --[0.30843702],  percep_loss --6.07349967956543, identity_loss --[[0.0271872]], identity_diff --[[0.0543744]]\n",
            "Training complete in 2m 59s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axso-SoZBLiV"
      },
      "source": [
        "! pip install deepface\r\n",
        "from deepface import DeepFace\r\n",
        " \r\n",
        "#face verification\r\n",
        "obj = DeepFace.verify(\"save_result/149.png\", \"source_image/3.png\", model_name = 'ArcFace')\r\n",
        "print(obj)\r\n",
        "#face verification\r\n",
        "obj = DeepFace.verify(\"save_result/149.png\", \"source_image/4.png\", model_name = 'ArcFace')\r\n",
        "print(obj)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}